# -*- coding: utf-8 -*-
"""
ê°€í†¨ë¦­ëŒ€ ê³µì§€ ì—ì´ì „íŠ¸ (URL ì¬ì‚¬ìš© + ê²€ìƒ‰ ë°±ì—…)
- searchURL.pyê°€ ë§Œë“  outputs/notice_links.csvì˜ normalized_urlì„ ìš°ì„  ì‚¬ìš©
- ì—†ìœ¼ë©´ ì œëª© ê²€ìƒ‰ â†’ ìƒì„¸ ë§í¬(articleNo) ì¶”ì¶œ(ì •ê·œí™”) â†’ ë³¸ë¬¸ ìˆ˜ì§‘ â†’ ì˜ì–´ ë²ˆì—­ â†’ DOCX ì €ì¥
- í…ìŠ¤íŠ¸ ì¤‘ì‹¬: ì´ë¯¸ì§€/í‘œ ì œì™¸

í•„ìš”:
  pip install playwright python-docx python-dotenv google-generativeai
  playwright install
  .env: GEMINI_API_KEY=..., GEMINI_MODEL=gemini-1.5-pro (ì˜µì…˜)
ì‹¤í–‰:
  python cuk_notice_agent.py
"""

from __future__ import annotations
import os, re, time, json, csv
from dataclasses import dataclass
from typing import Optional, List, Tuple, Dict
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from pathlib import Path

from dotenv import load_dotenv
from docx import Document
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout
import google.generativeai as genai

# --------------- ì„¤ì • ---------------
NOTICE_LIST_URL = "https://www.catholic.ac.kr/ko/campuslife/notice.do"
HEADLESS = True
BETWEEN_QUERIES_SLEEP = 0.8
NETWORK_IDLE_WAIT_MS = 1000
CSV_PATH = Path("outputs/notice_links.csv")   # searchURL.py ê²°ê³¼ íŒŒì¼

# CSVê°€ ì—†ì„ ë•Œë§Œ ì‚¬ìš©ë  ì œëª© ë¦¬ìŠ¤íŠ¸(ì˜ˆì‹œ)
TARGET_TITLES: List[str] = [
    "[ê±´ì¶•íŒ€] ê¹€ìˆ˜í™˜ê´€ ìŠ¹ê°•ê¸° 4í˜¸ê¸°(ì „ë§ìš© ìŠ¹ê°•ê¸° ì¤‘ ì¢Œì¸¡) ì œì–´ì‹œìŠ¤í…œ êµì²´ê³µì‚¬ ì•ˆë‚´",
    "[ëŒ€ì™¸í˜‘ë ¥íŒ€] í™ë³´ ì½˜í…ì¸  ì œì‘ ëŒ€ì™¸í˜‘ë ¥íŒ€ ì†Œì† ê¸°ê´€ë™ì•„ë¦¬ <CUKí”„ë Œì¦ˆ> 18ê¸° ëª¨ì§‘ ê³µê³ "
]

TRANSLATION_PROMPT_TEMPLATE = """You are a professional translator for university announcements.
Translate Korean to clear, natural, formal English.

Rules for Translation:
- Text-only output. Keep lists and section headings tidy.
- Translate 'Catholic University of Korea' to 'The Catholic University of Korea'.
- For building and department names, use the following official English names:
  * ì •ë¬¸: Main Gate
  * ì°½ì—…ë³´ìœ¡ì„¼í„°(BIê´€): Business Incubation Center(BI)
  * ê¹€ìˆ˜í™˜ê´€(Kê´€): Kim Sou-hwan Hall(K)
  * ìŠ¤í…ŒíŒŒë…¸ ê¸°ìˆ™ì‚¬(Kê´€): Dormitory Stephen(K)
  * ì•ˆë“œë ˆì•„ê´€(Aê´€): Andreas Dormitory(A)
  * ë§ˆë¦¬ì•„ê´€(Mê´€): Maria Hall(M)
  * ë‹ˆì½œìŠ¤ê´€(Nê´€): Nicholls Hall(N)
  * ë°¤ë¹„ë…¸ê´€(BAê´€): Bambino Hall(BA)
  * ë‹¤ì†”ê´€(Dê´€): Dasol Hall (Science Hall)(D)
  * ë¹„ë¥´íˆ¬ìŠ¤ê´€(Vê´€): Virtus Hall(V)
  * í•™ìƒë¯¸ë˜ì¸ì¬ê´€(Bê´€): Sophie Barat Hall(B)
  * í•˜ëŠ˜ë™ì‚°: Outdoor Stage
  * ë¯¸ì¹´ì—˜ê´€-í–‰ì •ë™(Hê´€): Michael Hall (Administration)(H)
  * ë¯¸ì¹´ì—˜ê´€-êµìˆ˜ì—°êµ¬ë™(Tê´€): Michael Hall (Faculty)(T)
  * ë² ë¦¬íƒ€ìŠ¤ê´€(ì¤‘ì•™ë„ì„œê´€): Veritas Hall (Central Library)(L)
  * ì„±ì‹¬ê´€(SHê´€): Songsim Hall(SH)
  * ì •ì§„ì„ì¶”ê¸°ê²½í•™ë¶€(NPê´€): Nicholas Cardinal Cheong Pharmacy Hall(NP)
  * ì½˜ì„œíŠ¸í™€(CHê´€): Concert Hall (Grand Auditorium)(CH)
  * ëŒ€ìš´ë™ì¥: Athletic Ground
  * ì˜ˆìˆ˜ì„±ì‹¬ì„±ë‹¹(Cê´€): Chapel of the Sacred Heart(C)
  * í”„ë€ì¹˜ìŠ¤ì½” ê¸°ìˆ™ì‚¬(Fê´€): Francisco Dormitory(F)
  * ê±´ì¶•íŒ€: Architecture Team
  * êµëª©ì‹¤: Office of Campus Ministry
  * êµìˆ˜í•™ìŠµê°œë°œì›: Center for Teaching & Learning
  * ê¸°ìˆ™ì‚¬ìš´ì˜íŒ€: Management Team of Dormitory
  * ëŒ€ì™¸í˜‘ë ¥íŒ€: External Affairs Team
  * ì‹œì„¤íŒ€: Facilities Management Team
  * ì´ë¬´íŒ€: General Affairs Team
  * ì·¨ì°½ì—…ì§€ì›íŒ€: Career Support Team
  * í‰ìƒêµìœ¡ì›: The Center for Lifelong Education
  * í•™ë¶€ëŒ€í•™ìš´ì˜íŒ€: College Management Team
  * í•™ì‚¬ì§€ì›íŒ€: Academic Affairs Services Team
  * í•™ìƒì§€ì›íŒ€: Student Affairs Team
  * ìœµí•©ì „ê³µí•™ë¶€: School of interdisciplinary studies
  * ì¤‘ì•™ë„ì„œê´€: The Catholic University of Korea Central Library
  * ì¸ê°„ì—°êµ¬ì†Œ: Research Institute of Anthropology
  * êµ¬ë§¤ê´€ì¬íŒ€: Purchasing and Property Management Team
  * ì°½ì—…êµìœ¡í˜ì‹ ì„¼í„°: Center for Entrepreneurship Education and Innovation
  * ì¼ë°˜ëŒ€í•™ì›(êµí•™íŒ€): General Graduate School (Team)
  * íŠ¹ìˆ˜ëŒ€í•™ì›(êµí•™íŒ€): Specialized Graduate School (Team)
  * êµìœ¡ëŒ€í•™ì›/êµíšŒë²•ëŒ€í•™ì›êµí•™íŒ€: Graduate School of Education/Graduate School of Canon Law
  * ê±´ì„¤ë³¸ë¶€: Construction Headquarters
  * ì •ë³´í†µì‹ ì›: Information & Communications Center
  * ê³ ìš©ë…¸ë™ë¶€: Ministry of Employment and Labor
  * í•œêµ­ê°€í†¨ë¦­êµ¿ë‰´ìŠ¤: Catholic Metaversity
  * ì¸ê¶Œì„¼í„°: Human Rights Center
- Preserve bullet/numbered lists and section structures, including 'â–³' or similar symbols. Do not rephrase them into a summary format.
- For "í•™ë³´ ê¸°ì‚¬" style announcements, translate 'ê¸€/ì‚¬ì§„:' to 'Text/Photos:'.
- Dates: keep original format and add weekday in parentheses if present (e.g., 2025.08.18 (Mon) ~ 2025.08.22 (Fri)).
- Remove UI crumbs like 'ëª©ë¡', 'ì´ì „ê¸€/ë‹¤ìŒê¸€', 'ì²¨ë¶€íŒŒì¼'.
- Do NOT add content not present in source. If unclear, write [unclear].

Original Korean text to translate:

{text_ko}
"""

# --------------- ë°ì´í„° êµ¬ì¡° ---------------
@dataclass
class FoundNotice:
    title_ko: str
    url_raw: Optional[str]
    url_norm: Optional[str]
    status: str
    note: str = ""
    date_text: Optional[str] = None
    body_text_ko: Optional[str] = None
    body_text_en: Optional[str] = None

# --------------- ìœ í‹¸ ---------------
def normalize_cuk_notice_url(url: str) -> str:
    p = urlparse(url)
    qs = parse_qs(p.query)
    article_no = qs.get("articleNo", [None])[0]
    if not article_no:
        return url
    new_qs = {"mode": "view", "articleNo": article_no}
    return urlunparse((p.scheme, p.netloc, p.path, p.params, urlencode(new_qs), ""))

def extract_article_no(url: str) -> Optional[str]:
    m = re.search(r"(?:[?&])articleNo=(\d+)", url)
    return m.group(1) if m else None

def load_urls_from_csv(path: Path) -> List[Dict[str, str]]:
    """
    notice_links.csv (columns: title,status,normalized_url,raw_url,note) ì½ê¸°
    normalized_url ìš°ì„ , ì—†ìœ¼ë©´ raw_url ì‚¬ìš©.
    """
    if not path.exists():
        return []
    rows = []
    with path.open("r", encoding="utf-8") as f:
        r = csv.DictReader(f)
        for row in r:
            url = (row.get("normalized_url") or "").strip() or (row.get("raw_url") or "").strip()
            if not url:
                continue
            rows.append({
                "title": (row.get("title") or "").strip(),
                "url": normalize_cuk_notice_url(url)
            })
    return rows

# --------------- DOM í—¬í¼ ---------------
def find_search_input(page):
    candidates = [
        "input[name='srSearchVal']",
        "input#srSearchVal",
        "input[type='search']",
        "input[placeholder*='ê²€ìƒ‰']",
        "input[title*='ê²€ìƒ‰']",
        "input[name*='search']",
    ]
    for sel in candidates:
        el = page.query_selector(sel)
        if el:
            return el
    els = page.query_selector_all("input")
    return els[0] if els else None

def click_first_result_and_get_url(page) -> Optional[str]:
    selectors = [
        "table tbody tr td a",        # í‘œ í˜•íƒœ
        "ul li a.title",              # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
        ".board-list a",              # ê³µí†µ ë³´ë“œ
        "a[href*='notice.do?mode=view']",
        "a[href*='articleNo=']",
    ]
    for sel in selectors:
        try:
            page.wait_for_selector(sel, timeout=5000)
            links = page.query_selector_all(sel)
            if links:
                with page.expect_navigation(timeout=15000):
                    links[0].click()
                return page.url
        except PWTimeout:
            continue
    return None

def extract_content_text(page) -> str:
    """
    ê°€í†¨ë¦­ëŒ€ ê³µì§€ ìƒì„¸ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ.
    - .fr-view (p/span êµ¬ì¡°) 1ìˆœìœ„ ì²˜ë¦¬
    - ë¶ˆí•„ìš” ìš”ì†Œ ì œê±° ë° &nbsp; ì •ê·œí™”
    - ì‹¤íŒ¨ ì‹œ í›„ë³´ ì»¨í…Œì´ë„ˆ / ìµœí›„ìˆ˜ë‹¨ í´ë°±
    """
    import re as _re

    # 0) ë¨¼ì € .fr-viewë¥¼ ê°•í•˜ê²Œ ì‹œë„ (ë„¤ê°€ ì¤€ DOM êµ¬ì¡°)
    fr = page.query_selector(".fr-view") or page.query_selector(".b-con-box .fr-view")
    if fr:
        raw = fr.evaluate(
            """
            (node) => {
              // ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°
              const clone = node.cloneNode(true);
              clone.querySelectorAll('script,style,.attach,.file,.btn,.share,.sns,.prev-next,.pagination').forEach(n => n.remove());

              const ps = Array.from(clone.querySelectorAll('p'));
              let lines = [];
              if (ps.length) {
                for (const p of ps) {
                  // p ì•ˆì˜ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (span ë“± í¬í•¨)
                  let t = p.innerText || "";
                  // NBSP -> space
                  t = t.replace(/\\u00A0/g, " ");
                  // ë¼ì¸ ë‹¨ìœ„ ë¶ˆí•„ìš” ê³µë°± ì •ë¦¬
                  t = t.replace(/[ \\t]+/g, " ").trim();
                  if (t) lines.push(t);
                }
              } else {
                let t = clone.innerText || "";
                t = t.replace(/\\u00A0/g, " ");
                t = t.replace(/[ \\t]+/g, " ").trim();
                if (t) lines.push(t);
              }
              return lines.join("\\n");
            }
            """
        ) or ""

        # í•˜ë‹¨ UI ì»·ì˜¤í”„ ë° ê³µë°± ì •ëˆ
        cut_words = ["ì²¨ë¶€íŒŒì¼", "ì´ì „ê¸€", "ë‹¤ìŒê¸€", "ëª©ë¡", "SNS", "ê³µìœ "]
        for w in cut_words:
            idx = raw.find(w)
            if idx != -1 and idx > 50:
                raw = raw[:idx].rstrip()
                break

        # ì—¬ëŸ¬ ë¹ˆ ì¤„ í•©ì¹˜ê¸°
        raw = _re.sub(r"\n{3,}", "\n\n", raw).strip()
        if len(raw) >= 30:
            return raw

    # 1) ì¼ë°˜ ì»¨í…Œì´ë„ˆ í›„ë³´ë“¤
    body_candidates = [
        "div.view-cont", "div.viewCont", "div.view-con",
        "div.board-view .cont", "div.board-view .content", "div.board-view",
        "div.board_view", "div.bv_cont", "div.bv-cont", "div.bvCon",
        "div#content", "div#boardContent", "div.article-body", "article.post",
        "#contents .view", "#contents .board", "#container .contents",
        "#contents .post-view", "#contents .bd_view", "#contents .view-con",
    ]
    for sel in body_candidates:
        el = page.query_selector(sel)
        if not el:
            continue
        txt = (el.inner_text() or "").replace("\u00A0", " ").strip()
        if len(txt) < 30:
            continue
        for w in ["ì²¨ë¶€íŒŒì¼", "ì´ì „ê¸€", "ë‹¤ìŒê¸€", "ëª©ë¡", "SNS", "ê³µìœ "]:
            idx = txt.find(w)
            if idx != -1 and idx > 50:
                txt = txt[:idx].rstrip()
                break
        txt = _re.sub(r"[ \t]+", " ", txt)
        txt = _re.sub(r"\n{3,}", "\n\n", txt).strip()
        if len(txt) >= 30:
            return txt

    # 2) ìµœí›„ìˆ˜ë‹¨: í˜ì´ì§€ì—ì„œ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡
    best_txt, best_len = "", 0
    for el in page.query_selector_all("main *, #contents *, #container *")[:2000]:
        try:
            t = (el.inner_text() or "").replace("\u00A0", " ").strip()
        except Exception:
            continue
        if len(t) > best_len:
            best_len, best_txt = len(t), t
    if best_txt:
        for w in ["ì²¨ë¶€íŒŒì¼", "ì´ì „ê¸€", "ë‹¤ìŒê¸€", "ëª©ë¡", "SNS", "ê³µìœ "]:
            idx = best_txt.find(w)
            if idx != -1 and idx > 50:
                best_txt = best_txt[:idx].rstrip()
                break
        best_txt = _re.sub(r"[ \t]+", " ", best_txt)
        best_txt = _re.sub(r"\n{3,}", "\n\n", best_txt).strip()
        return best_txt

    # 3) ë§ˆì§€ë§‰ìœ¼ë¡œ body ì „ì²´
    full = (page.inner_text("body") or "").replace("\u00A0", " ").strip()
    full = _re.sub(r"[ \t]+", " ", full)
    full = _re.sub(r"\n{3,}", "\n\n", full).strip()
    return full


# --------------- ë²ˆì—­ ---------------
def translate_ko_to_en(text_ko: str) -> str:
    load_dotenv()
    api_key = os.getenv("GEMINI_API_KEY")
    model_name = os.getenv("GEMINI_MODEL", "gemini-1.5-pro")
    if not api_key:
        return f"[No GEMINI_API_KEY provided â€“ original text follows]\n\n{text_ko}"

    genai.configure(api_key=api_key)
    try:
        model = genai.GenerativeModel(model_name=model_name)
    except Exception as e:
        return f"[Error: Failed to load model '{model_name}'. Details: {e}]\n\n{text_ko}"

    prompt = TRANSLATION_PROMPT_TEMPLATE.format(text_ko=text_ko)
    retries = 3
    for i in range(retries):
        try:
            resp = model.generate_content(prompt)
            return (resp.text or "").strip()
        except Exception as e:
            if "Quota exceeded" in str(e):
                print("Quota exceeded. Waiting for 60 seconds...")
                time.sleep(60)
                continue
            if i < retries - 1:
                print(f"Error calling Gemini API. Retrying in 2 seconds... ({e})")
                time.sleep(2)
            else:
                return f"[Error translating text after {retries} retries: {e}]\n\n{text_ko}"

# --------------- íŒŒì´í”„ë¼ì¸ ---------------
def fetch_from_url(page, title: str, url: str) -> FoundNotice:
    """ì •ê·œí™” URLì„ ì§ì ‘ ì—´ì–´ ë³¸ë¬¸ ì¶”ì¶œ/ë²ˆì—­"""
    item = FoundNotice(title_ko=title or "(no title)", url_raw=url, url_norm=normalize_cuk_notice_url(url), status="pending")
    try:
        print(f"\n--- Processing URL: {item.url_norm} ---")
        page.goto(item.url_norm, wait_until="domcontentloaded", timeout=20000)
        try:
            page.wait_for_load_state("networkidle", timeout=8000)
        except PWTimeout:
            pass
        page.wait_for_timeout(300)

        body_ko = extract_content_text(page)
        item.body_text_ko = body_ko or ""

        parts = [title] if title else []
        if item.body_text_ko:
            parts.append(item.body_text_ko)
        ko_bundle = "\n\n".join(parts).strip() or "(empty)"
        
        # í¬ë¡¤ë§ëœ í•œêµ­ì–´ ì›ë¬¸ ì¶œë ¥
        print("\nğŸ“„ Korean Original Text (bundle):")
        print("--------------------------------")
        print(ko_bundle)

        item.body_text_en = translate_ko_to_en(ko_bundle)
        item.status = "ok"
        item.note = ""

        # ë²ˆì—­ëœ ì˜ì–´ ë¬¸ì¥ ì¶œë ¥
        print("\n English Translation:")
        print("----------------------")
        print(item.body_text_en)

    except Exception as e:
        item.status = "error"
        item.note = f"{type(e).__name__}: {e}"
    return item

def fetch_by_search(page, title: str) -> FoundNotice:
    """CSVê°€ ì—†ì„ ë•Œ: ì œëª© ê²€ìƒ‰ìœ¼ë¡œ ë§í¬ íšë“ í›„ ë³¸ë¬¸/ë²ˆì—­"""
    item = FoundNotice(title_ko=title, url_raw=None, url_norm=None, status="pending")
    try:
        print(f"\n--- Searching for: {title} ---")
        page.goto(NOTICE_LIST_URL, wait_until="domcontentloaded", timeout=20000)
        page.wait_for_timeout(400)
        si = find_search_input(page)
        if not si:
            item.status = "fail"; item.note = "ê²€ìƒ‰ì°½ íƒìƒ‰ ì‹¤íŒ¨"; return item

        si.fill(""); si.type(title); si.press("Enter")
        try:
            page.wait_for_load_state("networkidle", timeout=10000)
        except PWTimeout:
            pass
        page.wait_for_timeout(NETWORK_IDLE_WAIT_MS)

        detail_url = click_first_result_and_get_url(page)
        if not detail_url:
            item.status = "not_found"; item.note = "ìƒì„¸ ë§í¬ ë¯¸ë°œê²¬"; return item

        item.url_raw = detail_url
        item.url_norm = normalize_cuk_notice_url(detail_url)

        try:
            page.wait_for_load_state("networkidle", timeout=8000)
        except PWTimeout:
            pass
        page.wait_for_timeout(300)

        body_ko = extract_content_text(page)
        item.body_text_ko = body_ko or ""

        parts = [title]
        if item.body_text_ko:
            parts.append(item.body_text_ko)
        ko_bundle = "\n\n".join(parts).strip()
        
        # í¬ë¡¤ë§ëœ í•œêµ­ì–´ ì›ë¬¸ ì¶œë ¥
        print("\nğŸ“„ Korean Original Text (bundle):")
        print("--------------------------------")
        print(ko_bundle)

        item.body_text_en = translate_ko_to_en(ko_bundle)
        item.status = "ok"
        item.note = ""
        
        # ë²ˆì—­ëœ ì˜ì–´ ë¬¸ì¥ ì¶œë ¥
        print("\nâœ… English Translation:")
        print("----------------------")
        print(item.body_text_en)

    except Exception as e:
        item.status = "error"
        item.note = f"{type(e).__name__}: {e}"
    return item

def fetch_and_translate() -> List[FoundNotice]:
    results: List[FoundNotice] = []
    url_rows = load_urls_from_csv(CSV_PATH)  # â† ìš°ì„  CSV ì‚¬ìš©
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=HEADLESS)
        page = browser.new_page()

        if url_rows:
            print(f"[info] Using URLs from: {CSV_PATH}")
            for r in url_rows:
                it = fetch_from_url(page, r.get("title", ""), r["url"])
                results.append(it)
                time.sleep(BETWEEN_QUERIES_SLEEP)
        else:
            print("[info] CSV not found. Falling back to title search.")
            for t in TARGET_TITLES:
                it = fetch_by_search(page, t)
                results.append(it)
                time.sleep(BETWEEN_QUERIES_SLEEP)

        browser.close()
    return results

def build_docx(items: List[FoundNotice], out_path: str):
    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    doc = Document()
    h = doc.add_heading("Selected Announcements â€“ English Compilation", 0)
    h.alignment = WD_ALIGN_PARAGRAPH.CENTER
    p = doc.add_paragraph("Source: Catholic University of Korea â€“ Campus Life > Notice")
    p.alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph("")

    for it in items:
        sec = doc.add_heading("", level=1); sec_run = sec.add_run(it.title_ko); sec_run.bold = True

        if it.url_norm:
            doc.add_paragraph(f"Original Link: {it.url_norm}")
        elif it.url_raw:
            doc.add_paragraph(f"Original Link (raw): {it.url_raw}")

        if it.status != "ok":
            doc.add_paragraph(f"Status: {it.status} ({it.note})")
            doc.add_paragraph(""); continue

        en = it.body_text_en or ""
        for line in en.splitlines():
            doc.add_paragraph(line)
        doc.add_paragraph("\nâ€”\n")

    for para in doc.paragraphs:
        for r in para.runs:
            r.font.size = Pt(11)

    doc.save(str(out_path))

def main():
    items = fetch_and_translate()

    Path("outputs").mkdir(exist_ok=True)
    with open("outputs/notice_results.json", "w", encoding="utf-8") as f:
        json.dump([vars(x) for x in items], f, ensure_ascii=False, indent=2)

    build_docx(items, "outputs/CUK_Announcements_EN.docx")
    print("\n Done: outputs/CUK_Announcements_EN.docx")
    for it in items:
        print(f"- [{it.status}] {it.title_ko} -> {it.url_norm or it.url_raw or 'N/A'} ({it.note})")

if __name__ == "__main__":
    main()
